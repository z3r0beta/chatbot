{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z3r0beta/chatbot/blob/main/AI_Kobold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to an L4 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "c6f8beee-a7dd-471d-d233-33f41176fc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "bac0ea29-9ff2-4131-c8fb-7849f7b06495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 8.0 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LostRuins/koboldcpp && cd koboldcpp"
      ],
      "metadata": {
        "id": "Bw7LPiRIrFpL",
        "outputId": "eed6afd8-1bb9-46d3-d8d1-415ddfaba969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'koboldcpp' already exists and is not an empty directory.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r koboldcpp/requirements.txt && pip install -e && cd ~/"
      ],
      "metadata": {
        "id": "Ov83rsWTsnE8",
        "outputId": "e82fbfeb-34ae-4d85-90d6-99e78eeb455b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.24.4 in ./anaconda3/lib/python3.12/site-packages (from -r koboldcpp/requirements.txt (line 1)) (1.26.4)\r\n",
            "Requirement already satisfied: sentencepiece>=0.1.98 in ./anaconda3/lib/python3.12/site-packages (from -r koboldcpp/requirements.txt (line 2)) (0.2.0)\r\n",
            "Requirement already satisfied: transformers>=4.34.0 in ./anaconda3/lib/python3.12/site-packages (from -r koboldcpp/requirements.txt (line 3)) (4.44.0)\n",
            "Collecting gguf>=0.1.0 (from -r koboldcpp/requirements.txt (line 4))\n",
            "  Downloading gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting customtkinter>=5.1.0 (from -r koboldcpp/requirements.txt (line 5))\n",
            "  Downloading customtkinter-5.2.2-py3-none-any.whl.metadata (677 bytes)\n",
            "Requirement already satisfied: protobuf>=4.21.0 in ./anaconda3/lib/python3.12/site-packages (from -r koboldcpp/requirements.txt (line 6)) (4.25.4)\n",
            "Requirement already satisfied: psutil>=5.9.4 in ./anaconda3/lib/python3.12/site-packages (from -r koboldcpp/requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: filelock in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (0.24.5)\n",
            "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (2024.7.24)\n",
            "Requirement already satisfied: requests in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (4.66.5)\n",
            "Collecting darkdetect (from customtkinter>=5.1.0->-r koboldcpp/requirements.txt (line 5))\n",
            "  Downloading darkdetect-0.8.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r koboldcpp/requirements.txt (line 3)) (2024.7.4)\n",
            "Downloading gguf-0.9.1-py3-none-any.whl (49 kB)\n",
            "Downloading customtkinter-5.2.2-py3-none-any.whl (296 kB)\n",
            "Downloading darkdetect-0.8.0-py3-none-any.whl (9.0 kB)\n",
            "Installing collected packages: gguf, darkdetect, customtkinter\n",
            "Successfully installed customtkinter-5.2.2 darkdetect-0.8.0 gguf-0.9.1\n",
            "\n",
            "Usage:   \n",
            "  pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip install [options] [-e] <vcs project url> ...\n",
            "  pip install [options] [-e] <local project path> ...\n",
            "  pip install [options] <archive url/path> ...\n",
            "\n",
            "-e option requires 1 argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YellowRoseCx/koboldcpp-rocm"
      ],
      "metadata": {
        "id": "1skYBxJXrQd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d74857-2874-430c-ffc4-ead85f0eaf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'koboldcpp-rocm'...\n",
            "remote: Enumerating objects: 33875, done.\u001b[K\n",
            "remote: Counting objects: 100% (7755/7755), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 33875 (delta 7660), reused 7526 (delta 7526), pack-reused 26120 (from 1)\u001b[K\n",
            "Receiving objects: 100% (33875/33875), 114.88 MiB | 4.54 MiB/s, done.\n",
            "Resolving deltas: 100% (24444/24444), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd koboldcpp-rocm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eq6PLMH1afU",
        "outputId": "e5f3f87a-9288-4eeb-d2a0-075dddf71db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/j0hnny/koboldcpp-rocm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "FEna_2KntODQ",
        "outputId": "bc74aec6-7c3e-40e9-8f49-ef187854cef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.24.4 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.26.4)\r\n",
            "Requirement already satisfied: sentencepiece>=0.1.98 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: transformers>=4.34.0 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.44.0)\n",
            "Requirement already satisfied: gguf>=0.1.0 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.9.1)\n",
            "Requirement already satisfied: customtkinter>=5.1.0 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (5.2.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: protobuf>=4.21.0 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (4.25.4)\n",
            "Requirement already satisfied: urllib3 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.9.4 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (6.0.0)\n",
            "Requirement already satisfied: filelock in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.24.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2024.7.24)\n",
            "Requirement already satisfied: requests in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: darkdetect in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from customtkinter>=5.1.0->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.34.0->-r requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.34.0->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/j0hnny/anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2024.7.4)\n",
            "/bin/bash: line 1: !pip: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc5JdT6T1Gzs",
        "outputId": "e59f92d8-866d-4b4d-9910-9f398895a63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " aiXcoder\t\t\t        libyui-master\r\n",
            " anaconda3\t\t\t        llama.cpp\r\n",
            " Android\t\t\t        main_katie-on-the-bench_spec_v2.png\r\n",
            " AndroidStudioProjects\t\t        main_spec_v2.json\r\n",
            " Applications\t\t\t        Music\r\n",
            " AWS\t\t\t\t        n.js\r\n",
            " aws-replication-installer-init         node_modules\r\n",
            " build\t\t\t\t        obj\r\n",
            " chatbot-ui\t\t\t        packages-microsoft-prod.deb\r\n",
            " Desktop\t\t\t        Pictures\r\n",
            " dillo-3.1.1\t\t\t        Program.cs\r\n",
            " Documents\t\t\t        Projects\r\n",
            " Downloads\t\t\t        Public\r\n",
            " git-lfs-3.5.1\t\t\t        Qt\r\n",
            " google-cloud-cli-linux-x86_64.tar.gz   Qt-Advanced-Docking-System\r\n",
            " google-cloud-sdk\t\t        risu-ai_123.0.0_amd64.AppImage\r\n",
            " huggingface\t\t\t        settings.json\r\n",
            " IDE\t\t\t\t        share\r\n",
            " j0hnny.csproj\t\t\t        Templates\r\n",
            " kimitzu\t\t\t        Videos\r\n",
            " KoboldAI\t\t\t        viper-browser\r\n",
            " koboldcpp\t\t\t       '~WeB~RooT~'\r\n",
            " koboldcpp-rocm\t\t\t        websites\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make LLAMA_OPENBLAS=1 LLAMA_CLBLAST=1 LLAMA_HIPBLAS=1 -j4"
      ],
      "metadata": {
        "id": "Lbj5n58_rXBF",
        "outputId": "dd8e4aff-e456-407e-82e0-c3ccf1c647e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \r\n",
            "I UNAME_S:  Linux\r\n",
            "I UNAME_P:  unknown\r\n",
            "I UNAME_M:  x86_64\r\n",
            "I CFLAGS:   -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native\r\n",
            "I CXXFLAGS: -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread\r\n",
            "I LDFLAGS:  \r\n",
            "I CC:       cc (GCC) 14.1.1 20240701 (Red Hat 14.1.1-7)\r\n",
            "I CXX:      g++ (GCC) 14.1.1 20240701 (Red Hat 14.1.1-7)\r\n",
            "\r\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c ggml/src/ggml.c -o ggml.o\r\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c otherarch/ggml_v3.c -o ggml_v3.o\r\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c otherarch/ggml_v2.c -o ggml_v2.o\r\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c otherarch/ggml_v1.c -o ggml_v1.o\r\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c expose.cpp -o expose.o\r\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c gpttype_adapter.cpp -o gpttype_adapter.o\r\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c otherarch/sdcpp/sdtype_adapter.cpp -o sdcpp_default.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c otherarch/whispercpp/whisper_adapter.cpp -o whispercpp_default.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c examples/llava/clip.cpp -o llavaclip_default.o\n",
            "In file included from \u001b[01m\u001b[Kexamples/llava/clip.cpp:24\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K./common/stb_image.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint\u001b[01;32m\u001b[K stbi__parse_png_file\u001b[m\u001b[K(stbi__png*, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./common/stb_image.h:5450:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 1 byte into a region of size 0 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            " 5450 |                         \u001b[01;35m\u001b[Ktc[k] = (stbi_uc)(stbi__get16be(s) & 255) *\u001b[m\u001b[K\n",
            "      |                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 5451 | \u001b[01;35m\u001b[K                                stbi__depth_scale_table[z->depth]\u001b[m\u001b[K; // non 8-bit images will be larger\n",
            "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./common/stb_image.h:5326:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 3 into destination object ‘\u001b[01m\u001b[Ktc\u001b[m\u001b[K’ of size 3\n",
            " 5326 |     stbi_uc has_trans = 0, \u001b[01;36m\u001b[Ktc\u001b[m\u001b[K[3] = {0};\n",
            "      |                            \u001b[01;36m\u001b[K^~\u001b[m\u001b[K\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c examples/llava/llava.cpp -o llava.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native -c ggml/src/ggml-backend.c -o ggml-backend_default.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native -c ggml/src/ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native -c ggml/src/ggml-aarch64.c -o ggml-aarch64.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c ggml/src/ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c src/unicode.cpp -o unicode.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c src/unicode-data.cpp -o unicode-data.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread  -c ggml/src/llamafile/sgemm.cpp -o sgemm.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c common/common.cpp -o common.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_OPENBLAS -DGGML_USE_BLAS -I/usr/local/include/openblas -c ggml/src/ggml.c -o ggml_v4_openblas.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_OPENBLAS -DGGML_USE_BLAS -I/usr/local/include/openblas -c otherarch/ggml_v3.c -o ggml_v3_openblas.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_OPENBLAS -DGGML_USE_BLAS -I/usr/local/include/openblas -c otherarch/ggml_v2.c -o ggml_v2_openblas.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_OPENBLAS -DGGML_USE_BLAS -I/usr/local/include/openblas -c gpttype_adapter.cpp -o gpttype_adapter_openblas.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -c ggml/src/ggml-blas.cpp -o ggml-blas.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_CLBLAST -c ggml/src/ggml.c -o ggml_v4_clblast.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_CLBLAST -c otherarch/ggml_v3.c -o ggml_v3_clblast.o\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -DGGML_USE_CLBLAST -c otherarch/ggml_v2.c -o ggml_v2_clblast.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_CLBLAST -c gpttype_adapter.cpp -o gpttype_adapter_clblast.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_CLBLAST -c ggml-opencl.cpp -o ggml-opencl.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_CLBLAST -c otherarch/ggml_v3-opencl.cpp -o ggml_v3-opencl.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_CLBLAST -c otherarch/ggml_v2-opencl.cpp -o ggml_v2-opencl.o\n",
            "cc -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native -c otherarch/ggml_v2-opencl-legacy.c -o ggml_v2-opencl-legacy.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native   -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c ggml/src/ggml.c -o ggml_v4_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native   -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c otherarch/ggml_v3.c -o ggml_v3_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -Ofast -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native   -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c otherarch/ggml_v2.c -o ggml_v2_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread  -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c gpttype_adapter.cpp -o gpttype_adapter_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread  -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c otherarch/sdcpp/sdtype_adapter.cpp -o sdcpp_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread  -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS  -c otherarch/whispercpp/whisper_adapter.cpp -o whispercpp_cublas.o\n",
            "g++ -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread  -c examples/llava/clip.cpp -o llavaclip_cublas.o\n",
            "In file included from \u001b[01m\u001b[Kexamples/llava/clip.cpp:24\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K./common/stb_image.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint\u001b[01;32m\u001b[K stbi__parse_png_file\u001b[m\u001b[K(stbi__png*, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./common/stb_image.h:5450:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kwriting 1 byte into a region of size 0 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            " 5450 |                         \u001b[01;35m\u001b[Ktc[k] = (stbi_uc)(stbi__get16be(s) & 255) *\u001b[m\u001b[K\n",
            "      |                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 5451 | \u001b[01;35m\u001b[K                                stbi__depth_scale_table[z->depth]\u001b[m\u001b[K; // non 8-bit images will be larger\n",
            "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./common/stb_image.h:5326:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kat offset 3 into destination object ‘\u001b[01m\u001b[Ktc\u001b[m\u001b[K’ of size 3\n",
            " 5326 |     stbi_uc has_trans = 0, \u001b[01;36m\u001b[Ktc\u001b[m\u001b[K[3] = {0};\n",
            "      |                            \u001b[01;36m\u001b[K^~\u001b[m\u001b[K\n",
            "cc  -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c11   -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-deprecated -Wno-deprecated-declarations -pthread -march=native -mtune=native  -c ggml/src/ggml-backend.c -o ggml-backend_cublas.o\n",
            "make: /usr/bin/hipconfig: No such file or directory\n",
            "Failed to 'dlopen' libhsa-runtime64.so\n",
            "Failed to load libamdhip64.so: libamdhip64.so: cannot open shared object file: No such file or directory\n",
            "/usr/bin/hipcc -I. -Iggml/include -Iggml/src -Iinclude -Isrc -I./common -I./include -I./include/CL -I./otherarch -I./otherarch/tools -I./otherarch/sdcpp -I./otherarch/sdcpp/thirdparty -I./include/vulkan -O3 -fno-finite-math-only -fmath-errno -DNDEBUG -std=c++11 -fPIC -DLOG_DISABLE_LOGS -D_GNU_SOURCE -DGGML_USE_LLAMAFILE -pthread -s -Wno-multichar -Wno-write-strings -Wno-deprecated -Wno-deprecated-declarations -pthread -DGGML_USE_HIPBLAS -DGGML_USE_CUDA -DSD_USE_CUBLAS   -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -x hip -c -o ggml-cuda.o ggml/src/ggml-cuda.cu\n",
            "make: /usr/bin/hipcc: No such file or directory\n",
            "make: *** [Makefile:276: ggml-cuda.o] Error 127\n",
            "make: *** Waiting for unfinished jobs....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 koboldcpp.py -h"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uh2_SUklvqKO",
        "outputId": "34f0cb91-d16e-4d59-b302-a36ed30f35ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***\r\n",
            "Welcome to KoboldCpp - Version 1.72.yr0-ROCm\n",
            "usage: koboldcpp.py [-h] [--model [filename]] [--port [portnumber]]\n",
            "                    [--host [ipaddr]] [--launch] [--config [filename]]\n",
            "                    [--threads [threads]] [--usecublas [[lowvram|normal]\n",
            "                    [main GPU ID] [mmq] [rowsplit] ...] | --usevulkan\n",
            "                    [[Device ID] ...] | --useclblast {0,1,2,3,4,5,6,7,8}\n",
            "                    {0,1,2,3,4,5,6,7,8} | --noblas]\n",
            "                    [--contextsize [256,512,1024,2048,3072,4096,6144,8192,12288,16384,24576,32768,49152,65536,98304,131072]]\n",
            "                    [--gpulayers [[GPU layers]]] [--tensor_split [Ratios]\n",
            "                    [[Ratios] ...]] [--checkforupdates]\n",
            "                    [--ropeconfig [rope-freq-scale] [[rope-freq-base] ...]]\n",
            "                    [--blasbatchsize {-1,32,64,128,256,512,1024,2048}]\n",
            "                    [--blasthreads [threads]] [--lora [lora_filename]\n",
            "                    [[lora_base] ...]] [--noshift] [--nommap] [--usemlock]\n",
            "                    [--noavx2] [--debugmode [DEBUGMODE]] [--skiplauncher]\n",
            "                    [--onready [shell command]] [--benchmark [[filename]]]\n",
            "                    [--multiuser [limit]] [--remotetunnel] [--highpriority]\n",
            "                    [--foreground] [--preloadstory PRELOADSTORY] [--quiet]\n",
            "                    [--ssl [cert_pem] [[key_pem] ...]] [--nocertify]\n",
            "                    [--mmproj MMPROJ] [--password PASSWORD] [--ignoremissing]\n",
            "                    [--chatcompletionsadapter CHATCOMPLETIONSADAPTER]\n",
            "                    [--flashattention] [--quantkv [quantization level 0/1/2]]\n",
            "                    [--forceversion [version]] [--smartcontext]\n",
            "                    [--unpack destination] [--hordemodelname [name]]\n",
            "                    [--hordeworkername [name]] [--hordekey [apikey]]\n",
            "                    [--hordemaxctx [amount]] [--hordegenlen [amount]]\n",
            "                    [--sdmodel [filename]] [--sdthreads [threads]]\n",
            "                    [--sdclamped [SDCLAMPED]] [--sdvae [filename] |\n",
            "                    --sdvaeauto] [--sdquant | --sdlora [filename]]\n",
            "                    [--sdloramult [amount]] [--whispermodel [filename]]\n",
            "                    [model_param] [port_param]\n",
            "\n",
            "KoboldCpp Server\n",
            "\n",
            "positional arguments:\n",
            "  model_param           Model file to load (positional)\n",
            "  port_param            Port to listen on (positional)\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model [filename]    Model file to load\n",
            "  --port [portnumber]   Port to listen on\n",
            "  --host [ipaddr]       Host IP to listen on. If empty, all routable\n",
            "                        interfaces are accepted.\n",
            "  --launch              Launches a web browser when load is completed.\n",
            "  --config [filename]   Load settings from a .kcpps file. Other arguments will\n",
            "                        be ignored\n",
            "  --threads [threads]   Use a custom number of threads if specified.\n",
            "                        Otherwise, uses an amount based on CPU cores\n",
            "  --usecublas [[lowvram|normal] [main GPU ID] [mmq] [rowsplit] ...]\n",
            "                        Use CuBLAS for GPU Acceleration. Requires CUDA. Select\n",
            "                        lowvram to not allocate VRAM scratch buffer. Enter a\n",
            "                        number afterwards to select and use 1 GPU. Leaving no\n",
            "                        number will use all GPUs. For hipBLAS binaries, please\n",
            "                        check YellowRoseCx rocm fork.\n",
            "  --usevulkan [[Device ID] ...]\n",
            "                        Use Vulkan for GPU Acceleration. Can optionally\n",
            "                        specify GPU Device ID (e.g. --usevulkan 0).\n",
            "  --useclblast {0,1,2,3,4,5,6,7,8} {0,1,2,3,4,5,6,7,8}\n",
            "                        Use CLBlast for GPU Acceleration. Must specify exactly\n",
            "                        2 arguments, platform ID and device ID (e.g.\n",
            "                        --useclblast 1 0).\n",
            "  --noblas              Do not use any accelerated prompt ingestion\n",
            "  --contextsize [256,512,1024,2048,3072,4096,6144,8192,12288,16384,24576,32768,49152,65536,98304,131072]\n",
            "                        Controls the memory allocated for maximum context\n",
            "                        size, only change if you need more RAM for big\n",
            "                        contexts. (default 4096). Supported values are [256,51\n",
            "                        2,1024,2048,3072,4096,6144,8192,12288,16384,24576,3276\n",
            "                        8,49152,65536,98304,131072]. IF YOU USE ANYTHING ELSE\n",
            "                        YOU ARE ON YOUR OWN.\n",
            "  --gpulayers [[GPU layers]]\n",
            "                        Set number of layers to offload to GPU when using GPU.\n",
            "                        Requires GPU. Set to -1 to try autodetect\n",
            "                        (experimental)\n",
            "  --tensor_split [Ratios] [[Ratios] ...]\n",
            "                        For CUDA and Vulkan only, ratio to split tensors\n",
            "                        across multiple GPUs, space-separated list of\n",
            "                        proportions, e.g. 7 3\n",
            "  --checkforupdates     Checks KoboldCpp-ROCm's release page on GitHub using\n",
            "                        HTTPS to see if there's a new update available.\n",
            "\n",
            "Advanced Commands:\n",
            "  --ropeconfig [rope-freq-scale] [[rope-freq-base] ...]\n",
            "                        If set, uses customized RoPE scaling from configured\n",
            "                        frequency scale and frequency base (e.g. --ropeconfig\n",
            "                        0.25 10000). Otherwise, uses NTK-Aware scaling set\n",
            "                        automatically based on context size. For linear rope,\n",
            "                        simply set the freq-scale and ignore the freq-base\n",
            "  --blasbatchsize {-1,32,64,128,256,512,1024,2048}\n",
            "                        Sets the batch size used in BLAS processing (default\n",
            "                        512). Setting it to -1 disables BLAS mode, but keeps\n",
            "                        other benefits like GPU offload.\n",
            "  --blasthreads [threads]\n",
            "                        Use a different number of threads during BLAS if\n",
            "                        specified. Otherwise, has the same value as --threads\n",
            "  --lora [lora_filename] [[lora_base] ...]\n",
            "                        LLAMA models only, applies a lora file on top of\n",
            "                        model. Experimental.\n",
            "  --noshift             If set, do not attempt to Trim and Shift the GGUF\n",
            "                        context.\n",
            "  --nommap              If set, do not use mmap to load newer models\n",
            "  --usemlock            Enables mlock, preventing the RAM used to load the\n",
            "                        model from being paged out. Not usually recommended.\n",
            "  --noavx2              Do not use AVX2 instructions, a slower compatibility\n",
            "                        mode for older devices.\n",
            "  --debugmode [DEBUGMODE]\n",
            "                        Shows additional debug info in the terminal.\n",
            "  --skiplauncher        Doesn't display or use the GUI launcher.\n",
            "  --onready [shell command]\n",
            "                        An optional shell command to execute after the model\n",
            "                        has been loaded.\n",
            "  --benchmark [[filename]]\n",
            "                        Do not start server, instead run benchmarks. If\n",
            "                        filename is provided, appends results to provided\n",
            "                        file.\n",
            "  --multiuser [limit]   Runs in multiuser mode, which queues incoming requests\n",
            "                        instead of blocking them.\n",
            "  --remotetunnel        Uses Cloudflare to create a remote tunnel, allowing\n",
            "                        you to access koboldcpp remotely over the internet\n",
            "                        even behind a firewall.\n",
            "  --highpriority        Experimental flag. If set, increases the process CPU\n",
            "                        priority, potentially speeding up generation. Use\n",
            "                        caution.\n",
            "  --foreground          Windows only. Sends the terminal to the foreground\n",
            "                        every time a new prompt is generated. This helps avoid\n",
            "                        some idle slowdown issues.\n",
            "  --preloadstory PRELOADSTORY\n",
            "                        Configures a prepared story json save file to be\n",
            "                        hosted on the server, which frontends (such as\n",
            "                        KoboldAI Lite) can access over the API.\n",
            "  --quiet               Enable quiet mode, which hides generation inputs and\n",
            "                        outputs in the terminal. Quiet mode is automatically\n",
            "                        enabled when running a horde worker.\n",
            "  --ssl [cert_pem] [[key_pem] ...]\n",
            "                        Allows all content to be served over SSL instead. A\n",
            "                        valid UNENCRYPTED SSL cert and key .pem files must be\n",
            "                        provided\n",
            "  --nocertify           Allows insecure SSL connections. Use this if you have\n",
            "                        cert errors and need to bypass certificate\n",
            "                        restrictions.\n",
            "  --mmproj MMPROJ       Select a multimodal projector file for LLaVA.\n",
            "  --password PASSWORD   Enter a password required to use this instance. This\n",
            "                        key will be required for all text endpoints. Image\n",
            "                        endpoints are not secured.\n",
            "  --ignoremissing       Ignores all missing non-essential files, just skipping\n",
            "                        them instead.\n",
            "  --chatcompletionsadapter CHATCOMPLETIONSADAPTER\n",
            "                        Select an optional ChatCompletions Adapter JSON file\n",
            "                        to force custom instruct tags.\n",
            "  --flashattention      Enables flash attention.\n",
            "  --quantkv [quantization level 0/1/2]\n",
            "                        Sets the KV cache data type quantization, 0=f16, 1=q8,\n",
            "                        2=q4. Requires Flash Attention, and disables context\n",
            "                        shifting.\n",
            "  --forceversion [version]\n",
            "                        If the model file format detection fails (e.g. rogue\n",
            "                        modified model) you can set this to override the\n",
            "                        detected format (enter desired version, e.g. 401 for\n",
            "                        GPTNeoX-Type2).\n",
            "  --smartcontext        Reserving a portion of context to try processing less\n",
            "                        frequently. Outdated. Not recommended.\n",
            "  --unpack destination  Extracts the file contents of the KoboldCpp binary\n",
            "                        into a target directory.\n",
            "\n",
            "Horde Worker Commands:\n",
            "  --hordemodelname [name]\n",
            "                        Sets your AI Horde display model name.\n",
            "  --hordeworkername [name]\n",
            "                        Sets your AI Horde worker name.\n",
            "  --hordekey [apikey]   Sets your AI Horde API key.\n",
            "  --hordemaxctx [amount]\n",
            "                        Sets the maximum context length your worker will\n",
            "                        accept from an AI Horde job.\n",
            "  --hordegenlen [amount]\n",
            "                        Sets the maximum number of tokens your worker will\n",
            "                        generate from an AI horde job.\n",
            "\n",
            "Image Generation Commands:\n",
            "  --sdmodel [filename]  Specify a stable diffusion safetensors or gguf model\n",
            "                        to enable image generation.\n",
            "  --sdthreads [threads]\n",
            "                        Use a different number of threads for image generation\n",
            "                        if specified. Otherwise, has the same value as\n",
            "                        --threads.\n",
            "  --sdclamped [SDCLAMPED]\n",
            "                        If specified, limit generation steps and resolution\n",
            "                        settings for shared use. Accepts an extra optional\n",
            "                        parameter that indicates maximum resolution (eg. 768\n",
            "                        clamps to 768x768, min 512px, disabled if 0).\n",
            "  --sdvae [filename]    Specify a stable diffusion safetensors VAE which\n",
            "                        replaces the one in the model.\n",
            "  --sdvaeauto           Uses a built-in VAE via TAE SD, which is very fast,\n",
            "                        and fixed bad VAEs.\n",
            "  --sdquant             If specified, loads the model quantized to save\n",
            "                        memory.\n",
            "  --sdlora [filename]   Specify a stable diffusion LORA safetensors model to\n",
            "                        be applied. Cannot be used with quant models.\n",
            "  --sdloramult [amount]\n",
            "                        Multiplier for the LORA model to be applied.\n",
            "\n",
            "Whisper Transcription Commands:\n",
            "  --whispermodel [filename]\n",
            "                        Specify a Whisper bin model to enable Speech-To-Text\n",
            "                        transcription.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~/content/j0hnny/"
      ],
      "metadata": {
        "id": "gCQ-IvUEwDY2",
        "outputId": "e103618e-d7d3-4d2f-e971-0ee97d7d03de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/home/j0hnny/content/j0hnny/'\n",
            "/home/j0hnny/koboldcpp-rocm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29VxLtXW27bE",
        "outputId": "4a6032c9-7fa8-4c1c-8acf-dffb1574b64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build-info.h\t\t       kcpp_docs.embd\r\n",
            "class.py\t\t       kcpp_sdui.embd\r\n",
            "clblast.dll\t\t       klite.embd\r\n",
            "CLINFO_LICENSE\t\t       koboldcpp.py\r\n",
            "CMakeLists.txt\t\t       koboldcpp.sh\r\n",
            "colab.ipynb\t\t       lib\r\n",
            "common\t\t\t       libopenblas.dll\r\n",
            "common.o\t\t       LICENSE.md\r\n",
            "convert_hf_to_gguf.py\t       llavaclip_cublas.o\r\n",
            "convert_hf_to_gguf_update.py   llavaclip_default.o\r\n",
            "convert_llama_ggml_to_gguf.py  llava.o\r\n",
            "convert_lora_to_gguf.py        Makefile\r\n",
            "cudart64_110.dll\t       make_pyinstaller.bat\r\n",
            "cudart64_12.dll\t\t       make_pyinstaller_cuda12.bat\r\n",
            "easy_KCPP-ROCm_install.sh      make_pyinstaller_cuda.bat\r\n",
            "environment.yaml\t       make_pyinstaller_cuda_oldcpu.bat\r\n",
            "examples\t\t       make_pyinstaller_exe_rocm_only.bat\r\n",
            "expose.cpp\t\t       make_pyinstaller.sh\r\n",
            "expose.h\t\t       make_pyinst_rocm_hybrid_henk_yellow.bat\r\n",
            "expose.o\t\t       media\r\n",
            "ggml\t\t\t       MIT_LICENSE_GGML_LLAMACPP_ONLY\r\n",
            "ggml-aarch64.o\t\t       model_adapter.cpp\r\n",
            "ggml-alloc.o\t\t       model_adapter.h\r\n",
            "ggml-backend_cublas.o\t       msvcp140_codecvt_ids.dll\r\n",
            "ggml-backend_default.o\t       msvcp140.dll\r\n",
            "ggml-blas.o\t\t       mypy.ini\r\n",
            "ggml.o\t\t\t       nikogreen.ico\r\n",
            "ggml-opencl.cpp\t\t       niko.ico\r\n",
            "ggml-opencl.h\t\t       OpenCL.dll\r\n",
            "ggml-opencl.o\t\t       otherarch\r\n",
            "ggml-quants.o\t\t       poetry.lock\r\n",
            "ggml_v1.o\t\t       pyproject.toml\r\n",
            "ggml_v2_clblast.o\t       pyrightconfig.json\r\n",
            "ggml_v2_cublas.o\t       README.md\r\n",
            "ggml_v2.o\t\t       Remote-Link.cmd\r\n",
            "ggml_v2_openblas.o\t       requirements\r\n",
            "ggml_v2-opencl-legacy.o        requirements.txt\r\n",
            "ggml_v2-opencl.o\t       rwkv_vocab.embd\r\n",
            "ggml_v3_clblast.o\t       rwkv_world_vocab.embd\r\n",
            "ggml_v3_cublas.o\t       sampling.o\r\n",
            "ggml_v3.o\t\t       sdcpp_cublas.o\r\n",
            "ggml_v3_openblas.o\t       sdcpp_default.o\r\n",
            "ggml_v3-opencl.o\t       sgemm.o\r\n",
            "ggml_v4_clblast.o\t       simpleclinfo.cpp\r\n",
            "ggml_v4_cublas.o\t       src\r\n",
            "ggml_v4_openblas.o\t       taesd.embd\r\n",
            "gguf-py\t\t\t       taesd_xl.embd\r\n",
            "glslc.exe\t\t       unicode-data.o\r\n",
            "gpttype_adapter_clblast.o      unicode.o\r\n",
            "gpttype_adapter.cpp\t       vcruntime140_1.dll\r\n",
            "gpttype_adapter_cublas.o       vcruntime140.dll\r\n",
            "gpttype_adapter.o\t       vulkan-1.dll\r\n",
            "gpttype_adapter_openblas.o     whispercpp_cublas.o\r\n",
            "grammar-parser.o\t       whispercpp_default.o\r\n",
            "include\t\t\t       winclinfo.exe\r\n",
            "kcpp_adapters\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l45VjQqL3Kl7",
        "outputId": "37359a19-1cff-46c7-a18f-e8833ca741b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/'\n",
            "/home/j0hnny/koboldcpp-rocm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "New-yN8o3i5c",
        "outputId": "e3f0efb8-c3fb-4200-a7bf-87dbeefb5006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build-info.h\t\t       kcpp_docs.embd\r\n",
            "class.py\t\t       kcpp_sdui.embd\r\n",
            "clblast.dll\t\t       klite.embd\r\n",
            "CLINFO_LICENSE\t\t       koboldcpp.py\r\n",
            "CMakeLists.txt\t\t       koboldcpp.sh\r\n",
            "colab.ipynb\t\t       lib\r\n",
            "common\t\t\t       libopenblas.dll\r\n",
            "common.o\t\t       LICENSE.md\r\n",
            "convert_hf_to_gguf.py\t       llavaclip_cublas.o\r\n",
            "convert_hf_to_gguf_update.py   llavaclip_default.o\r\n",
            "convert_llama_ggml_to_gguf.py  llava.o\r\n",
            "convert_lora_to_gguf.py        Makefile\r\n",
            "cudart64_110.dll\t       make_pyinstaller.bat\r\n",
            "cudart64_12.dll\t\t       make_pyinstaller_cuda12.bat\r\n",
            "easy_KCPP-ROCm_install.sh      make_pyinstaller_cuda.bat\r\n",
            "environment.yaml\t       make_pyinstaller_cuda_oldcpu.bat\r\n",
            "examples\t\t       make_pyinstaller_exe_rocm_only.bat\r\n",
            "expose.cpp\t\t       make_pyinstaller.sh\r\n",
            "expose.h\t\t       make_pyinst_rocm_hybrid_henk_yellow.bat\r\n",
            "expose.o\t\t       media\r\n",
            "ggml\t\t\t       MIT_LICENSE_GGML_LLAMACPP_ONLY\r\n",
            "ggml-aarch64.o\t\t       model_adapter.cpp\r\n",
            "ggml-alloc.o\t\t       model_adapter.h\r\n",
            "ggml-backend_cublas.o\t       msvcp140_codecvt_ids.dll\r\n",
            "ggml-backend_default.o\t       msvcp140.dll\r\n",
            "ggml-blas.o\t\t       mypy.ini\r\n",
            "ggml.o\t\t\t       nikogreen.ico\r\n",
            "ggml-opencl.cpp\t\t       niko.ico\r\n",
            "ggml-opencl.h\t\t       OpenCL.dll\r\n",
            "ggml-opencl.o\t\t       otherarch\r\n",
            "ggml-quants.o\t\t       poetry.lock\r\n",
            "ggml_v1.o\t\t       pyproject.toml\r\n",
            "ggml_v2_clblast.o\t       pyrightconfig.json\r\n",
            "ggml_v2_cublas.o\t       README.md\r\n",
            "ggml_v2.o\t\t       Remote-Link.cmd\r\n",
            "ggml_v2_openblas.o\t       requirements\r\n",
            "ggml_v2-opencl-legacy.o        requirements.txt\r\n",
            "ggml_v2-opencl.o\t       rwkv_vocab.embd\r\n",
            "ggml_v3_clblast.o\t       rwkv_world_vocab.embd\r\n",
            "ggml_v3_cublas.o\t       sampling.o\r\n",
            "ggml_v3.o\t\t       sdcpp_cublas.o\r\n",
            "ggml_v3_openblas.o\t       sdcpp_default.o\r\n",
            "ggml_v3-opencl.o\t       sgemm.o\r\n",
            "ggml_v4_clblast.o\t       simpleclinfo.cpp\r\n",
            "ggml_v4_cublas.o\t       src\r\n",
            "ggml_v4_openblas.o\t       taesd.embd\r\n",
            "gguf-py\t\t\t       taesd_xl.embd\r\n",
            "glslc.exe\t\t       unicode-data.o\r\n",
            "gpttype_adapter_clblast.o      unicode.o\r\n",
            "gpttype_adapter.cpp\t       vcruntime140_1.dll\r\n",
            "gpttype_adapter_cublas.o       vcruntime140.dll\r\n",
            "gpttype_adapter.o\t       vulkan-1.dll\r\n",
            "gpttype_adapter_openblas.o     whispercpp_cublas.o\r\n",
            "grammar-parser.o\t       whispercpp_default.o\r\n",
            "include\t\t\t       winclinfo.exe\r\n",
            "kcpp_adapters\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "24a4e261-0efd-4cce-f34b-e5e0b49e09a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/bin/nvidia-smi\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LLaVAmmproj \u001b[38;5;129;01mand\u001b[39;00m LoadLLaVAmmproj:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️"
          ]
        }
      ],
      "source": [
        "#@title <b>v-- Enter your model below and then click this to start Koboldcpp</b>\n",
        "\n",
        "Model = \"https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_S.gguf\" #@param [\"https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_S.gguf\",\"https://huggingface.co/KoboldAI/LLaMA2-13B-Estopia-GGUF/resolve/main/LLaMA2-13B-Estopia.Q4_K_S.gguf\",\"https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-GGUF/resolve/main/Fimbulvetr-11B-v2.Q4_K_S.gguf\",\"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_M.gguf\",\"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q4_K_M.gguf\",\"https://huggingface.co/TheBloke/Xwin-LM-13B-v0.2-GGUF/resolve/main/xwin-lm-13b-v0.2.Q4_K_M.gguf\",\"https://huggingface.co/mradermacher/mini-magnum-12b-v1.1-GGUF/resolve/main/mini-magnum-12b-v1.1.Q4_K_S.gguf\",\"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q4_K_M.gguf\",\"https://huggingface.co/TheBloke/MythoMax-L2-Kimiko-v2-13B-GGUF/resolve/main/mythomax-l2-kimiko-v2-13b.Q4_K_M.gguf\",\"https://huggingface.co/TheBloke/MistRP-Airoboros-7B-GGUF/resolve/main/mistrp-airoboros-7b.Q4_K_S.gguf\",\"https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF/resolve/main/airoboros-mistral2.2-7b.Q4_K_S.gguf\",\"https://huggingface.co/concedo/KobbleTinyV2-1.1B-GGUF/resolve/main/KobbleTiny-Q4_K.gguf\",\"https://huggingface.co/grimjim/kukulemon-7B-GGUF/resolve/main/kukulemon-7B.Q8_0.gguf\",\"https://huggingface.co/mradermacher/LemonKunoichiWizardV3-GGUF/resolve/main/LemonKunoichiWizardV3.Q4_K_M.gguf\",\"https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q4_K_M-imatrix.gguf\",\"https://huggingface.co/mradermacher/L3-8B-Stheno-v3.2-i1-GGUF/resolve/main/L3-8B-Stheno-v3.2.i1-Q4_K_M.gguf\",\"https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix/resolve/main/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q4_K_M-imat.gguf\",\"https://huggingface.co/bartowski/NeuralDaredevil-8B-abliterated-GGUF/resolve/main/NeuralDaredevil-8B-abliterated-Q4_K_M.gguf\",\"https://huggingface.co/bartowski/L3-8B-Lunaris-v1-GGUF/resolve/main/L3-8B-Lunaris-v1-Q4_K_M.gguf\",\"https://huggingface.co/mradermacher/L3-Umbral-Mind-RP-v2.0-8B-GGUF/resolve/main/L3-Umbral-Mind-RP-v2.0-8B.Q4_K_M.gguf\"]{allow-input: true}\n",
        "Layers = 99 #@param [99]{allow-input: true}\n",
        "ContextSize = 4096 #@param [4096] {allow-input: true}\n",
        "#@markdown <hr>\n",
        "LoadLLaVAmmproj = True #@param {type:\"boolean\"}\n",
        "LLaVAmmproj = \"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\" #@param [\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/mistral-7b-mmproj-v1.5-Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-7b-mmproj-v1.5-Q4_0.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/LLaMA3-8B_mmproj-Q4_1.gguf\"]{allow-input: true}\n",
        "VCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadImgModel = True #@param {type:\"boolean\"}\n",
        "ImgModel = \"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\"]{allow-input: true}\n",
        "SCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadSpeechModel = True #@param {type:\"boolean\"}\n",
        "SpeechModel = \"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\" #@param [\"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\"]{allow-input: true}\n",
        "WCommand = \"\"\n",
        "\n",
        "import os\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "\n",
        "%cd /content\n",
        "if LLaVAmmproj and LoadLLaVAmmproj:\n",
        "  VCommand = \"--mmproj vmodel.gguf\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if ImgModel and LoadImgModel:\n",
        "  SCommand = \"--sdmodel imodel.gguf --sdthreads 4 --sdquant --sdclamped\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if SpeechModel and LoadSpeechModel:\n",
        "  WCommand = \"--whispermodel wmodel.bin\"\n",
        "else:\n",
        "  WCommand = \"\"\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "# simple fix for a common URL mistake\n",
        "if \"https://huggingface.co/\" in Model and \"/blob/main/\" in Model:\n",
        "    Model = Model.replace(\"/blob/main/\", \"/resolve/main/\")\n",
        "!aria2c -x 10 -o model.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Model\n",
        "if VCommand:\n",
        "  !aria2c -x 10 -o vmodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $LLaVAmmproj\n",
        "if SCommand:\n",
        "  !aria2c -x 10 -o imodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $ImgModel\n",
        "if WCommand:\n",
        "  !aria2c -x 10 -o wmodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $SpeechModel\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $ContextSize --quiet --remotetunnel $VCommand $SCommand $WCommand\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}